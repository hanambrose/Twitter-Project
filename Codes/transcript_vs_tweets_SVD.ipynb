{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "transcript vs tweets_SVD.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBTXeNZU_kLu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import WhitespaceTokenizer \n",
        "from collections import Counter\n",
        "from nltk import bigrams\n",
        "from nltk.stem import PorterStemmer\n",
        "from textblob import TextBlob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import PCA\n",
        "from scipy.sparse.linalg import svds\n",
        "from sklearn.preprocessing import normalize\n",
        " \n",
        "#mount your Google drive into this notebook\n",
        "drive.mount('/content/gdrive')\n",
        "#find the path to your Google drive root\n",
        "os.getcwd()+\"/gdrive/My Drive\"\n",
        "path = os.chdir('/content/gdrive/My Drive/Colab Notebooks/Data Mining/Group Project')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06gUJ8DGAFVw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "punctuation = list(string.punctuation)\n",
        "stop = stopwords.words('english') + punctuation\n",
        "\n",
        "def clean_txt(df, size): #prefer this method\n",
        "    tweets_text = df['text'].str.replace(r'[^\\x00-\\x7F]+', '', regex=True) # remove non-character word    \n",
        "    clean_tweets = []\n",
        "    for t in tweets_text: # loop through every tweets\n",
        "        terms_stop = [re.sub(r\"\\d\", \"\", term, flags=re.I) for term in WhitespaceTokenizer().tokenize(t) # remove term that contains number\n",
        "                    if term.lower() \n",
        "                    not in stop\n",
        "                    #and not term.isdigit()\n",
        "                    and not term.startswith(('@','#','http','rt', 'via'))]     \n",
        "        terms_stop = list(filter(None, terms_stop)) # remove empty/ non_character words        \n",
        "        terms_stem = [PorterStemmer().stem(word) for word in terms_stop] \n",
        "        if len(terms_stop) > size: # only analyze long tweets          \n",
        "          t=\" \".join(terms_stop)\n",
        "          #t=\" \".join(terms_stop)        \n",
        "          clean_tweets.append(t)\n",
        "    return clean_tweets\n",
        "\n",
        "def cand_data(df,cand_name):\n",
        "    df['candidate'] = np.where(df['text'].str.contains(cand_name, flags=re.IGNORECASE,regex=True),'Filtered', '')\n",
        "    df = df[df.candidate =='Filtered']\n",
        "    return df['text']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPNusWFuFe8J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def svd_func(df,word_in,top_sim):\n",
        "  clean_tweets = clean_txt(df, 20) # only analyze length of more than 20 words \n",
        "  vectorizer= TfidfVectorizer()\n",
        "  my_matrix = vectorizer.fit_transform([t for t in clean_tweets]).transpose() # words x documents(tweets)\n",
        "  print(my_matrix.shape)\n",
        "  words_compressed, _, docs_compressed = svds(my_matrix, k=40) # most data stays in the first 10 dimension, choose 40 to be safe\n",
        "  docs_compressed = docs_compressed.transpose()\n",
        "  print(words_compressed.shape)\n",
        "  print(docs_compressed.shape) \n",
        "\n",
        "  word_to_index = vectorizer.vocabulary_\n",
        "  index_to_word = {i:t for t,i in word_to_index.items()}\n",
        "\n",
        "  words_compressed = normalize(words_compressed, axis = 1) # PCA\n",
        "\n",
        "  k= top_sim # number of most closest words\n",
        "  if word_in not in word_to_index: return \"Not in vocab.\"\n",
        "  sims = words_compressed.dot(words_compressed[word_to_index[word_in],:]) # U x vector of the search word to find similarity\n",
        "  asort = np.argsort(-sims)[:k+1] # sorting similarity from biggest to smallest, that's why there is a negative sign\n",
        "  return [(index_to_word[i],sims[i]/sims[asort[0]]) for i in asort[1:]] # return similar words, similarity"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KOlcmQeKOtZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests\n",
        "import bs4\n",
        "\n",
        "#URL = \"https://www.rev.com/blog/transcripts/january-iowa-democratic-debate-transcript\"\n",
        "URL = \"https://www.rev.com/blog/transcripts/democratic-debate-transcript-las-vegas-nevada-debate\"\n",
        "\n",
        "\n",
        "requests.get(URL, {}).text\n",
        "web_page = bs4.BeautifulSoup(requests.get(URL, {}).text, \"lxml\")\n",
        "transcript = web_page.body.find_all(name=\"p\")\n",
        "\n",
        "# Remove links to video\n",
        "for i in range(len(transcript)):\n",
        "  transcript[i] = re.sub(r\"\\(<a .*</a>\\)<br/>\", '', transcript[i].text)\n",
        "transcript = pd.DataFrame(transcript,columns=['text'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zh5LfzX4H7j3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "outputId": "d8271450-5b71-4568-a5aa-107bdb383bb5"
      },
      "source": [
        "svd_func(transcript, 'trump',10)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2161, 171)\n",
            "(2161, 40)\n",
            "(171, 40)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('donald', 0.9266385017198637),\n",
              " ('beat', 0.7840938355251859),\n",
              " ('board', 0.7205891109308151),\n",
              " ('eight', 0.7205891109308151),\n",
              " ('ahead', 0.7205891109308151),\n",
              " ('interesting', 0.7205891109308151),\n",
              " ('equipped', 0.7205891109308151),\n",
              " ('wonderful', 0.7205891109308151),\n",
              " ('toss', 0.7205891109308151),\n",
              " ('poll', 0.5700803400612271)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdSaSb77Q864",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "outputId": "b84b3722-0a29-4fec-e076-2912cf5d9f5b"
      },
      "source": [
        "twitter_1 = pd.read_csv('public_database.csv')\n",
        "trump_twt_1 = svd_func(twitter_1, 'trump',10)\n",
        "trump_twt_1"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(11738, 2565)\n",
            "(11738, 40)\n",
            "(2565, 40)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('hairs', 0.8268590302303673),\n",
              " ('unhealthy', 0.7673640050911901),\n",
              " ('bloated', 0.7673640050911901),\n",
              " ('incoherently', 0.7673640050911901),\n",
              " ('wrinkled', 0.7673640050911901),\n",
              " ('invest', 0.7393690666029499),\n",
              " ('caucused', 0.7337167805822422),\n",
              " ('alot', 0.7158926394673832),\n",
              " ('chuck', 0.6574975150363199),\n",
              " ('coping', 0.6556045285963331)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOutYr8QKXpQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "outputId": "f92310c3-efc1-430f-e8d1-7d5ccc1378ba"
      },
      "source": [
        "twitter_2 = pd.read_csv('public_database-0220.txt')\n",
        "trump_twt_2 = svd_func(twitter_2, 'trump',10)\n",
        "trump_twt_2"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(14429, 3672)\n",
            "(14429, 40)\n",
            "(3672, 40)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('beats', 0.8002747196496772),\n",
              " ('tops', 0.7772178833263852),\n",
              " ('sworn', 0.7576827080866215),\n",
              " ('knowledgable', 0.7473786214563808),\n",
              " ('evasive', 0.7473786214563808),\n",
              " ('uttered', 0.7405985267472185),\n",
              " ('lashes', 0.7405985267472185),\n",
              " ('hispanic', 0.7405985267472185),\n",
              " ('longest', 0.7061147822722741),\n",
              " ('sheds', 0.7000258355117607)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    }
  ]
}